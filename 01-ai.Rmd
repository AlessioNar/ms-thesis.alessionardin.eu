# Artificial Intelligence {#chap-ai}

Artificial Intelligence technologies (AI) are increasingly becoming part of our daily life. They have become commonly used in academia and the business world alike, in a large range of fields, allowing the application of notions from both natural and social sciences. While AI is based on mathematics and computer science, its recent successes derive from a combination of three fundamental factors: the increasing availability of large amounts of data, growing computing power, and new and more efficient algorithms.

## What is AI?

Although there are many different definitions of what exactly is the aim of AI studies[^1], essentially the development of AI systems focuses on building machines capable of performing tasks that typically require human intelligence. On a more practical level, when we speak of modern AI systems we generally refer to prediction machines. AI systems have no cognitive state: [@Searle1990MindsBA] they simply are machines that, given some inputs, return some more or less determined outputs. We generally speak of AI because these machines are capable of returning accurate answers even when provided limited and partial information, thus predicting. AI systems exploit previously collected data to create a statistical model of the world capable of predicting an output corresponding to our expectations with a reasonable amount of accuracy. In this framework, intelligence (in its general meaning) is not involved at all.

[^1]: @russell2010artificial identify four main definitions of artificial intelligence: (1) systems that think like humans; (2) systems that act like humans; (3) systems that think rationally; (4) systems that act rationally. According to @haenlein2019brief, this lack of agreement is due to two factors: first, it would require agreeing on how to define human intelligence; second, there is little evidence that proves how much machine intelligence resembles human intelligence.

In this context, some experts distinguish between Artificial Narrow Intelligence (ANI) and Artificial General Intelligence (AGI). ANI, or weak AI, involves AI systems specialized in performing a particular activity, such as playing chess, recognizing images, or transcribing a speech. As we are all aware, the diffusion of such systems has rapidly made these instrument parts of the essential toolkits of our daily life, automating repetitive tasks and increasing efficiency.

On the other hand, with AGI, or strong AI, scholars identify a machine capable to autonomously perform any intellectual task that a human can perform, in other words, an artificially intelligent system capable of achieving a broad and general objective, using a model of reality to make predictions and plan actions [@flowers2019strong]. Many scholars, above all @Searle1990MindsBA and @dreyfus1972computers, think that this objective is not achievable, nor desirable.[^2]

[^2]: Leaving aside the risks of such systems, there are important ethical implications with no easy answer.

## An economic perspective of AI

Since modern AI technologies represent a drastic improvement of prediction technology [@agrawal2018prediction], let us consider the role of prediction in the decision-making process. According to @agrawal2018prediction, the decision making process can be split into the stages of prediction and judgment, where prediction is the act of mapping the possible outcomes of a variety of actions and judgment is the act of choosing the most well-suited path to reach an objective [@agrawal2018predictionA].

In economic terms, given a predetermined state $S$, prediction consists of mapping all the set of actions $X$ that can undertake, and their respective outcomes. Judgment, on the other hand, consists of the process of evaluating the outcome obtained from choosing an action $\chi$ from a set of possible actions $X$. The payoff for each action can be defined as a utility function $\mu$ ($\chi$, $\theta$), where $\theta$ is the realization of an uncertain state drawn from a distribution F($\theta$) [@agrawal2018predictionA]. The decision-making process is thus reduced to a *"standard problem of choice under uncertainty"* [@agrawal2018predictionA] where prediction maps the likelihood of possible outputs and judgment ranks the desirability of each output. Since it is assumed that utility can only be determined by humans, who have to undertake a costly process that allows the mapping from ($\chi$, $\theta$) to a specific payoff value $\mu$, the utility function is unknown to the AI. Evaluation requires time and effort, and it is strictly linked to the operator's final goal, which is not easily translatable to a machine.[^3]

[^3]: As @agrawal2018predictionA affirmed, *"We assume that this process of determining payoffs requires human understanding of the situation: it is not a prediction problem"*.

AI technologies thus have substantially reduced the costs associated with prediction. However, while in some cases this has led to the full automation of simple tasks, in others is unlikely that this will lead to substitution of human users, rather to the enhancement of human capabilities. In economic terms, this equals asking if judgment and prediction are complements or substitutes. @agrawal2018predictionA created an model to determine whether prediction can reach a level of accuracy able to entirely take over the judgment part, concluding that they are complements, provided that judgment is not too difficult. When complexity is introduced, *"the impact of improved prediction on the value of judgment depends on whether improved prediction leads to automated decision making"* [@agrawal2018predictionA]. As complexity increases, there is a tendency for humans to opt for default decisions and heuristics that, on average, perform well. This aspect reduces the economic value of judgment and increases the one of prediction at the margin. @agrawal2018predictionA analyzed complex problems such as automation, contracting, and firm boundaries. They suggest that the possibility of introducing automated decision-making through prediction largely depends on the specific features of the tasks, with the value of judgment increasing when the cost of mistakes is particularly high, as in the case of applications of AI to tasks that have a great impact of people's life, such as the automatic detection of tax fraud.

## A Brief History of AI

As a formal science, AI was born less than 80 years ago, drawing from a variety of different fields, such as philosophy, mathematics, linguistics, psychology, economics, and many others [@russell2010artificial]. Providing a complete review of the topic goes beyond the scope of the dissertation, therefore in the following pages, I will only present a brief summary of the milestones of modern AI.

### Early developments: 1940s -- 1950s

In 1943, McCulloch and Pitts published the first work that is now generally considered AI [@McCulloch1943ALC]. By combining the basic physiology of neurons, the formal analysis of propositional logic, and Turing's theory of computation, they demonstrated that networks of connected neurons could compute any computable function. This work is notably the forerunner of both the logicist and the connectionist tradition of AI. Subsequently, other scholars started working on these concepts, and in 1951 Minsky and Edmonds built the first neural network computer [@russell2010artificial]. In 1950, Alan Turing published the seminal paper *"Computing Machinery and Intelligence"*, where he describes the process of creating intelligent machines and proposes a method of testing machine intelligence through a "game of imitation", where a human should be able to distinguish a human from an AI in a teletype dialogue. [^4]. In 1956, John McCarthy organized a workshop at Dartmouth College that brought together all the significant figures of the growing AI field. While the workshop itself did not lead to any notable discovery, it made all these researchers know each other, fundamentally contributing to the evolution of the AI field [@russell2010artificial].

[^4]: A method that was later renamed as the "Turing Test"

### High expectations and the first AI winter: 1960s -- 1970s

In the following years, the AI field experienced the first successes, such as the creation of the General Problem Solver (GPS),[^5]. These early programs often contained little or no knowledge of their subject matter and often succeeded in using simple syntactic manipulations (as in the firsts machine translation efforts). It was a general belief that solving more complex problems was only a matter of faster hardware and larger memories, an assumption proven wrong afterwards [@russell2010artificial]. This progressively led to what has been described as the first AI winter. The high expectations that the public had for AI gradually faded away, and in 1974, criticism by Sir James Lighthill on AI technologies added to the pressure from public research institutions to fund more productive projects, leading to the cut of AI exploratory research by the US and British governments [@gonsalves2019summers].

[^5]: A system that applied human problem-solving protocols to solve simple problems the development of the high-level language LISP, and the invention of time-sharing systems for optimizing the use of computing resources @nilsson2009quest

### Expert systems 1980s -- 1990s

The development of the first microprocessors at the end of the 1970s renewed interest in AI research, leading to the development of the DENDRAL program, an expert system that capable of inferring the molecular structure based on the information provided by a mass spectrometer [@nilsson2009quest]. Its expertise derived from large numbers of special-purpose rules collected through the extensive interviewing of domain experts. In the 1980s, expert systems gained commercial success, and by 1985, the AI market had reached a value of over a million dollars [@russell2010artificial]. Moreover, when Japan launched the Fifth Generation Computer Project, US and UK researchers feared that Japan could establish supremacy in the field and managed to generate a similar investment in the US [@ray1998case]. At the end of the 1980s, the first limitations of expert systems emerged. Their programming was extremely complex, and as the number of rules increased, a "black box" effect made the functioning of the machine hard to understand. Development and maintenance were challenging, and therefore faster, easier, and less expensive ways to achieve the same results progressively took over [@russell2010artificial]. The last success of expert system was Deep Blue, IBM's AI that in 1997 beat the world chess champion Garry Kasparov. Nevertheless, as all expert system, it was based on a brute force algorithm that evaluated each move based on a set of rules extracted from chess books and previous grandmaster games [@campbell2002deep], and marked the abandonment of rule-based AI systems. Even a relatively simple task such as playing chess required setting a complex set of parameters, limiting their applicability to the vast complexity of the world [@nilsson2009quest].

### Data + Computer power = Machine Learning 2000s -- 2010s

In the late 1990s and early 2000s, increased computational power and data availability gave new impetus to AI research. A focus on the resolution of closed and specific problems and new ties between AI and other fields led to increased use of AI for logistics, data mining, and medical diagnosis: AI entered into the mainstream technological use [@haenlein2019brief].

The development of the internet and the dramatic increase in data availability drastically improved machine learning algorithms' performance. Unlike expert systems, where the programmer had to explicitly tell the machine how to act in individual situations, machine learning systems build a mathematical model based on input data. While the first learning systems were developed in the early 1960s, their performance improved only after large sets of training data became available [@nilsson2009quest]. Machine learning is not about writing a set of instructions to perform a specific task but rather letting the machine determine the best prediction model for a particular problem, based on massive amounts of data and a limited number human-set parameters. Quality training sets became more important, marking a paradigm shift from the previous approach that focused on algorithms optimization [@russell2010artificial]. The larger and more complex the training set is, the more computing power is required. Computer graphics card processors (capable of performing multiple parallel computing, called GPUs) allowed to build affordable ML models and, therefore, increased AI researchers' data processing capabilities [@lee2018techology]. In the past years, machine learning has been used for performing more complex tasks, within the framework of narrow and clearly defined problems, such as board games, text classification, and image recognition. In 2011, Watson, the question-answering AI system powered by IBM, repeatedly won against two "Jeopardy!" champions [@ferrucci2012introduction], while in 2012 Google X finally managed to have an AI that can recognize various objects (such as cats and dogs) in videos without any explicit programming and in 2016, AlphaGO beat the world champion of the Chinese game Go [@borowiec2016alphago]. More recently, deep learning (a subfield of machine learning based on multiple layers of artificial neural networks) has increased in popularity among AI engineers [@goodfellow2016deep]. Deep learning is now commonly used for machine translation, image recognition, drug design, and many other fields, but it still presents some limitations, such as a "black box" effect that prevents human engineers from understanding how and why a final model does or does not work.

### Two different approaches to AI: symbolism and connectionism

Since the beginning of the formal study of AI it is possible to discern two different schools of thought which influenced how the AI field developed: Symbolism and Connectionism.

The symbolist tradition of AI is based on the idea that intelligence can be achieved through the manipulation of symbols[^6] [@newell2007computer], where symbols are a high-level abstraction of objects, problems and logic. They provide the AI with an already codified representation of knowledge, using a top-down approach. A typical example of such systems are expert systems, which emulate a human expert's decision-making ability by codifying it in a series of if-then rules by drawing from a previously acquired knowledge base[^7]. These systems present the advantage that once the basic shell is developed, it is possible to instill knowledge through a format that is easily comprehensible to domain experts, reducing the cost of intervention of IT experts. Additionally, it is possible to obtain a prototype in a relatively short amount of time. However, the process of knowledge acquisition to achieve a valuable product may result in being extremely long [@greene1987automated\], and as the size of the knowledge base increases, the harder it becomes to verify the consistency of the decisions, making prioritization and ambiguities common challenges of sizable expert systems. Finally, when it becomes necessary to update the knowledge base, the risk of disrupting the system persists, since every change in if-clause statements might potentially endanger the whole system \[@partridge1987scope\]. Some scholars even affirmed that expert systems that are not able to learn should not even be considered AI \[@schank1983current]. While many agree with this proposition, they represented a milestone in the development of the field, and are generally considered as AI for historical reasons [@partridge1987scope].

[^6]: This is also called the "physical symbol systems" hypothesis [@newell1980physical]

[^7]: From a structural perspective, they are divided into two subsystems: the knowledge base (that contains facts and rules) and the inference engine (that combines rules and known facts to deduce unknown facts).

The connectionist (or sub-symbolist) tradition adopts another approach, affirming that to achieve intelligence AI systems need to mimic the functioning of the human brain rather than just manipulating abstractions. In the words of @Searle1990MindsBA: *"a computer has syntax but not semantics."* Sub-symbolic systems have a bottom-up approach, starting from the lowest level and building intelligence by *connecting the dots*. Machine learning (ML) is an example of such technologies, which allows AI systems to learn from examples without hard-coding the instructions, but rather adjusting a set of parameters in order to achieve a desirable outcome. ML optimizes a statistical model defined up to some parameters based on example data or past experiences, also called training data [@alpaydin2004introduction]. The model can be either predictive, descriptive, or both. Assuming that the data is accurate and the model was constructed correctly, ML systems are able to detect patterns and schemes with acceptable approximation, sometimes even outperforming human experts [@grace2018will]. Performance greatly relies on the training set: the more examples a model is given during training, the more information it has when it comes to making accurate predictions and correctly classifying unseen cases [@russell2010artificial], [@jordan2015machine].

The main techniques used to build ML models can be classified in supervised, unsupervised, and reinforcement learning \[@alpaydin2004introduction\]. In supervised learning, the training data is labeled, meaning that a sample of input vectors (inputs) is coupled with a sample of corresponding target vectors (outputs).[^8] Unsupervised learning instead focuses on identifying patterns in the training set where no labeling or feedback is provided [@dey2016machine]. It is mainly used in cluster analysis, detecting *"potentially useful clusters of input examples"* [@russell2010artificial]. The underlying assumption is that the structural properties (algebraic, combinatorial, or probabilistic) of data might provide insights essential to the prediction [@jordan2015machine]. Finally, reinforcement learning maximizes a payoff function based on the actions or decisions that the AI system takes. labeled training examples are provided to indicate whether a decision in an unknown dynamic environment is correct or not for a given input [@jordan2015machine].[^9]. While this classification is helpful to understand the dynamics underlying machine learning, in most real-life applications, AI models are often trained using mixed strategies (generally referred to as semi-supervised learning).

[^8]: Common algorithms used in supervised learning are linear and logistic regression, na√Øve Bayes, support vector machines, and K-nearest-neighbour \[@kotsiantis2007supervised\].

[^9]: An example of the output of a payoff function might be game scores: they can be given at every step, as in a ping-pong game, or only in the end, as in chess @russell2010artificial

Recently, Deep Learning, a ML technique that uses multiple layers of artificial neural networks, has proven effective in solving many complex problems. An artificial neural network is a collection of connected artificial neurons that mimic the behavior of biological neurons. An artificial neuron is a simple processing unit designed to be trained from data (through unsupervised induction) [@henderson2010]. In an artificial neural network, each neuron receives an incoming signal and transmits it to the neurons it is connected to. Typically, neurons are aggregated into layers that may perform different transformations on their inputs [@russell2010artificial]. These kinds of machine learning structures have been applied to many fields such as computer vision, speech recognition, natural language processing, audio recognition, and board game programs, where they reached results that could be compared to human experts and that, in some cases, outperformed them [@pouyanfar2018survey]. Unfortunately, training this kind of structure is very complex, and deep learning algorithms often operate as "black boxes", since their complexity makes it difficult to ML engineers to understand their functioning [@castelvecchi2016can].

## Some AI capabilities

Often, media and popular culture depicts AI systems in the form of gigantic supercomputers, which seem far from our human experience. However, reality contradicts these colorful narratives. Different applications (such as search engines, microtargeting advertisement, news aggregators, recommendation systems, speech recognition, and so on so forth), products and services that use AI technology are entering our daily lives, providing alternative solutions to traditional tasks. With no pretense of completeness, I retained useful to provide a non-exhaustive list of different AI capabilities, along the lines of human abilities: seeing, listening, understanding, thinking, and moving.

### Seeing: Computer Vision

Computer vision is a field of AI that aims to understand the content of visual inputs. It may involve extracting a description, either in the form of a text, an object, or a three-dimensional model. In other words, *"at an abstract level, the goal of computer vision problems is to use the observed image data to infer something about the world" \[*@prince2012computer\]. Computer vision technologies acquire, process, analyze, and understand visual inputs to extract high-dimensional data from the real world to produce numerical and symbolic information [@rosenfeld1988computer].

The main tasks operated by computer vision systems are recognition, motion analysis, scene reconstruction, and image restoration. Image recognition determines whether or not image data contains a specific object, feature, or activity [@forsyth2012computer]. Applications of AI spans from optical character recognition (OCR) to pose estimation and facial recognition [@nilsson2009quest]. Motion analysis instead processes an image sequence (a video) to estimate the direction and velocity of an object (tracking) or the camera itself (ego-motion). When tracking and ego-motion technologies are combined, they are generally referred to as optical flow [@russell2010artificial]. Scene reconstruction deals with the construction of a model of a 3-dimensional space. Its application ranges from crime reconstructions to the digitization of cultural heritage and geospatial mapping [@trucco1998introductory]. Finally, image reconstruction is used to restore old or damaged visual content, removing noise, and augmenting quality [@banham1997digital]. In this way, it is possible to obtain more precise image data that can later be subjected to human analysis or other computer vision techniques.

### Listening: Speech recognition

Similar to image recognition, speech recognition systems transcribe speech to text. Audio signals are cleaned and processed to isolate the frequencies that represent a human voice. Then, with the help of linguistic models, a final text is produced [@ashri2020core]. Speech analysis can provide additional information regarding the speaker, such as identity, emotional state, health status, accent, and gender [@nassif2019speech]. Speech-to-text technology represents a crucial asset for transforming orally recorded data into text data that can later be analyzed using Natural Language Processing (NLP) techniques, further diminishing information retrieval's transaction cost. Unfortunately, the performance of speech-to-text technologies is subjected to high variance depending on the settings in which the recording takes place. Various studies show how even when dealing with systems that have a high-quality output in controlled environments, if disturbing elements[^10] are introduced, their performance goes below the human benchmark.[^11]

[^10]: Such as noise @gupta2016efficient, foreign accents @eskenazi1999using, and children's voices @potamianos1997automatic.

[^11]: Poor performance may also depend on training with insufficient data of adverse situations. It is possible that if noisy data is provided during the training process, the model performance will improve @catania2019automatic.

### Understanding: Natural Language Processing

Natural Language Processing (NLP) is a field of AI concerned with the interactions between computers and human (natural) languages, particularly with how to program computers to process and analyze large amounts of natural language data [@NaturalLangChowd]. In the first days of NLP, the typical approach was based on expert systems, where grammatical features and heuristics were hard-coded [@nilsson2009quest]. With the introduction of machine learning techniques the performance of linguistic models drastically improved. NLP is now used in fields such as *"machine translation, natural language text processing, and summarization, user interfaces, multilingual and cross-language information retrieval (CLIR), speech recognition, artificial intelligence, and expert systems, and so on"* [@NaturalLangChowd].

Whoever attempts to build software capable of extracting and manipulating natural language generally encounters three significant issues: the extraction of the thought process, the storage of the representation and meaning of the linguistic output, and the contextualization of information [@NaturalLangChowd]. As a consequences, approaches may vary: some are based on lexical and morphological analysis (Part-Of-Speech tagging), others on semantic and discourse analysis (Semantic Role Labeling, chunking), and knowledge-based approaches (Named Entity Recognition). On a more practical level, NLP systems generally comprehends a combination of all of these, starting from the word level, extending it to the sentence level, and finally framing it in the context of the specific domain [@NaturalLangChowd].

Some typical NLP applications that are currently being investigated are concept extraction, machine translation, question answering, and natural language generation \[@clark2013handbook.\] Among those, concept extraction is the most problematic: while in some cases NLP systems showed promising results in very restricted domains, we do not know yet how to correctly compute the meaning of a sentence based on words and context [@chai2001role]. Machine translation is one of the earliest applications of NLP, but it still presents several complex problems since human language is ambiguous and is characterized by a large number of exceptions. Nevertheless, recently machine translation has reached a stage that allows people to enjoy its benefits [@NaturalLangChowd]. Even if it is not always perfect and the translations are not as good as humans', the initial results are very encouraging, and machine translations may serve as a way for human translators to speed up the process and improve their performance. Question-answering systems and natural language generators are showing promising results [^12].

[^12]: In particular, the autoregressive linguistic model GPT-3, developed by OpenAI and introduced in May 2020, with an incredibly large number of 175 billion parameters, can generate fiction, poetry, newspaper articles, programming code, and probably much more. At the time of writing, this kind of technology might prove to be a game-changer in NLP development.

Despite these advances, three challenges prevent NLP from reaching commercial success: scalability, portability and variability. NLP systems establish patterns that are valid only for a specific domain and a particular task. When the topic, context, or user, change, it is necessary to create entirely new patterns. Second, advanced NLP techniques such as concept extraction are too computationally extensive for large scale NLP applications [@jones1999role]. Third, human behavior and communication patterns are erratic and constantly evolving, while the NLP system requires extensive and stable corpora to produce effective results [@NaturalLangChowd].

### Strategic Thinking: AI Planning

A fundamental functional application of AI system is AI planning, which aims to build systems capable to design the set of actions to perform to achieve a desired goal. Planning involves the introduction to AI systems of concepts such as time, causality, intentions, uncertainty, and multiple agents' actions. The classical formulation of the planning problem requires three inputs: a description of the state of the world, a description of the agent goal, and a description of the possible actions that can be performed (also called domain theory). The planner then outputs a sequence of actions designed to achieve that goal [@weld1999recent].

Recently there has been a peak in the interest in what has been called domain-independent planning (when no input domain is specified). Now machine learning algorithms are used to improve planners' speed and quality performance, but there are still many challenges. The high uncertainty of the domain makes *"completing a plan a significantly more difficult task than computing one"* [@LEONETTI2016103]. Relevant details and dynamics might be misinterpreted, creating imperfect models. While current systems manage to scale up antecedent problems, planning in uncertain situations still needs further research to reach acceptable results [@jimenez2012review].

### Moving: Robotics

While the previous AI applications can entirely be performed in digital environments, robotics involves the interaction of AI systems with the physical world, where uncertainty and external actors increase complexity [@brady1985artificial]. It is important to stress that robotics does not necessarily involve AI: we can distinguish between intelligent and non-intelligent robots. Non-intelligent robots are mechanical devices that perform operations based on instructions that are either hard-coded into their systems or transmitted to them by humans through a telecommunication infrastructure, while an intelligent robot is *"a mechanical creature which can function autonomously"* [@Murphy2000IntroductionTA]. While in the early days of AI, robotics was considered an integral part of the field, it progressively diverged over the years. Robotics focused more and more on the manufacturing industry and the assembly line's automation, which required no intelligence to function. Some contact points remained, but it was relegated to applications where humans could not efficiently communicate with robots in any way, such as in space explorations [@Murphy2000IntroductionTA]. Recently, advances in both fields have brought renewed interest in closing the gap between them. Robots are increasingly introduced in *"less engineered and more open environments. To do so, they need to rely on cognitive capabilities typical of AI, such as knowledge representation, learning, adaptation, and human-robot interaction "* [@rajan2017towards]. Increase autonomy of robotic systems may prove to be particularly useful in applications where humans are at significant risk (such as in space, military, or health threats) or in trivial, physically harsh, and unpleasant tasks (such as in the service industry or agriculture).

On the other hand, intelligent robots' introduction raises ethical concerns of a different scale than other AI applications, especially in robot-human interaction. The deployment of Lethal Autonomous Weapon Systems (LAWS) has increased concerns in the international community. Still, even apparent innocuous intelligent robots such as autonomous vehicles present ethical dilemmas in contexts where there is neither a human nor time available to provide an answer (e.g. in the context of a car crash). In these cases, the determination of liability is not trivial and there is no shared position in the academic community [@Lin2011RobotET].

## Conclusions

In this chapter, I presented AI technologies from both an historical and a technical perspective. In particular, I clarified a common misconceptions regarding AI technology: AI systems are not intelligent machines and even if the development of AGI prove to be possible, it is not likely it will happen in the short or medium term. While AI systems may seem to perform intelligent actions or to reason intelligently, they do possess an internal cognitive state regarding the actions they are performing [@Searle1990MindsBA], making them ontologically indistinguishable from other software. The recent advances in AI can be circumscribed as improvements in prediction technologies, one of the critical elements of every decision-making process, but certainly not the only one. Framing AI as a prediction technology is key to understand why and how AI technologies are becoming pervasive in the economy. The progressive digitization of society (which improves the process of data gathering across various dimension of the human experience) increases the opportunities to apply AI technologies in the most diverse areas, thus expanding the range of possible innovations.
